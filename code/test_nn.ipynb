{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-17T16:21:17.377487Z",
     "start_time": "2024-11-17T16:21:17.373520Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv2d, MaxPool2d\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Linear\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T16:21:19.131717Z",
     "start_time": "2024-11-17T16:21:19.126234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Wisdom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wisdom,self).__init__()\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=6,kernel_size=3,stride=1,padding=0)\n",
    "        # 3 为输入通道数，6 为输出通道数,表示6个卷积核进行处理得到六个输出，3 为卷积核大小\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=3,ceil_mode=False)\n",
    "        self.relu1 =ReLU() #默认 inplace=False，即不改变原数据\n",
    "        self.linear1 = Linear(196608,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        return x\n",
    "    def maxpool(self,x):\n",
    "        output = self.maxpool1(x)\n",
    "        return output\n",
    "    def relu(self,x):\n",
    "        output = self.relu1(x)\n",
    "        return output\n",
    "    def linear(self,x):\n",
    "        output = self.linear1(x)\n",
    "        return output"
   ],
   "id": "413b6896392c0b",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "学习module的使用，定义一个类",
   "id": "2562ca321bc6031f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:40:49.165007Z",
     "start_time": "2024-11-13T16:40:49.159498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#input 为输入图像\n",
    "input = torch.tensor([[1,2,0,3,1],\n",
    "                      [0,1,2,3,1],\n",
    "                      [1,2,1,0,0],\n",
    "                      [5,2,3,1,1],\n",
    "                      [2,1,0,1,1]])\n",
    "\n",
    "# kernel 为卷积核，\n",
    "kernel = torch.tensor([[1,2,1],\n",
    "                       [0,1,0],\n",
    "                       [2,1,0]])\n",
    "\n",
    "input = torch.reshape(input,(1,1,5,5))  # 1 为batch_size, 1 为通道数\n",
    "# batch_size指一次训练迭代中，模型处理的样本数量，通道数表示特征图的数量\n",
    "\n",
    "kernel = torch.reshape(kernel,(1,1,3,3))  \n",
    "\n",
    "output = F.conv2d(input, kernel, stride=1) # stride为步长,表示卷积核每次移动的距离\n",
    "print(output)\n",
    "\n",
    "output2 = F.conv2d(input, kernel, stride=2) # 卷积核每次移动的距离为2\n",
    "print(output2)\n",
    "\n",
    "output3 = F.conv2d(input, kernel, stride=1, padding=1) # padding为填充, 1表示在图像周围填充一圈0,再进行卷积操作\n",
    "print(output3)"
   ],
   "id": "9219527f97968a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[10, 12, 12],\n",
      "          [18, 16, 16],\n",
      "          [13,  9,  3]]]])\n",
      "tensor([[[[10, 12],\n",
      "          [13,  3]]]])\n",
      "tensor([[[[ 1,  3,  4, 10,  8],\n",
      "          [ 5, 10, 12, 12,  6],\n",
      "          [ 7, 18, 16, 16,  8],\n",
      "          [11, 13,  9,  3,  4],\n",
      "          [14, 13,  9,  7,  4]]]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 学习卷积操作\n",
    "$f(t)g(x-t)dt$ $f(t)$是变化函数，给定的输入，$g(x-t)$是卷积核，代表对周围参数的影响，卷积核在输入上滑动，计算卷积核与输入的乘积\n",
    "\n",
    "#### 卷积核作用：\n",
    "<ol>\n",
    "<li>对不稳定输入进行稳定输出，例如$f(x)$表示摄入食品与时间的关系，$g(x)$表示食品消化的量与时间的关系，对两个相乘函数求积分，代表胃容物总量与时间关系</li>\n",
    "<li>图像经过卷积核处理，表示周围像素点如何对当前像素产生影响，从而进行平滑处理</li>\n",
    "<li>看作对周围像素点的试探，从而得到更好的特征提取</li>\n",
    "</ol>"
   ],
   "id": "5a47452daf39724"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T09:09:05.319375Z",
     "start_time": "2024-11-17T09:08:51.119191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = torchvision.datasets.CIFAR10(root='./dataset', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "writer = SummaryWriter(\"logs\")\n",
    "step=0\n",
    "wisdom = Wisdom()\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    output = wisdom(imgs)\n",
    "    print(imgs.shape)\n",
    "    print(output.shape)\n",
    "    # torch.Size([64, 3, 32, 32])\n",
    "    writer.add_images(\"input\",imgs,step)\n",
    "    # torch.Size([64, 6, 30, 30]) -> [xxx,3,30,30] 因为彩色图像有3个通道，6个通道会报错，所以需要reshape\n",
    "    # 多余部分进入batch_size\n",
    "    output = torch.reshape(output,(-1,3,30,30))\n",
    "    writer.add_images(\"output\",output,step)\n",
    "    step = step + 1\n"
   ],
   "id": "d312342c55f708f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64, 6, 30, 30])\n",
      "torch.Size([16, 3, 32, 32])\n",
      "torch.Size([16, 6, 30, 30])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T09:14:09.536519Z",
     "start_time": "2024-11-17T09:14:03.975344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# maxpool 用法\n",
    "input = torch.tensor([[1,2,0,3,1],\n",
    "                      [0,1,2,3,1],\n",
    "                      [1,2,1,0,0],\n",
    "                      [5,2,3,1,1],\n",
    "                      [2,1,0,1,1]])\n",
    "input = torch.reshape(input,(-1,1,5,5))\n",
    "print (input.shape)\n",
    "output = wisdom.maxpool(input)\n",
    "print(output)\n",
    "\n",
    "writer = SummaryWriter(\"logs_maxpool\")\n",
    "step=0\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    output = wisdom.maxpool(imgs)\n",
    "    writer.add_images(\"input\",imgs,step)\n",
    "    writer.add_images(\"output\",output,step)\n",
    "    step = step + 1"
   ],
   "id": "e6d2bcd787711dc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 5])\n",
      "tensor([[[[2]]]])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 最大池化操作\n",
    "池化核在输入上滑动，每次取池化核覆盖区域的最大值，得到输出\n",
    "\n",
    "默认stride=kernel_size,即池化核每次移动的距离为池化核大小\n",
    "\n",
    "若最后一次滑动不足以覆盖整个区域，则依照Ceil_model,若为true，保留最后一次滑动的最大值；若为false，舍弃最后一次滑动的最大值\n",
    "\n",
    "作用：用于减少数据维度和参数量，提高训练速度"
   ],
   "id": "53b7b385df695bd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T15:27:16.002834Z",
     "start_time": "2024-11-17T15:27:15.998672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# relu 用法\n",
    "input = torch.tensor([[1,-0.5],\n",
    "                      [-1,3]])\n",
    "input = torch.reshape(input,(-1,1,2,2))\n",
    "wisdom = Wisdom()\n",
    "output = wisdom.relu(input)\n",
    "print(output)"
   ],
   "id": "7eb6d1ee0a8f78e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0.],\n",
      "          [0., 3.]]]])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T16:23:04.468437Z",
     "start_time": "2024-11-17T16:23:03.426080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# linear 操作\n",
    "dataset = torchvision.datasets.CIFAR10(root='./dataset', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "wisdom= Wisdom()\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    # output = torch.reshape(imgs,(1,1,1,-1))\n",
    "    output = torch.flatten(imgs) #展平成一行\n",
    "    output = torch.flatten(imgs)\n",
    "    output = wisdom.linear1(output)\n",
    "    print (output.shape)"
   ],
   "id": "586311ba1febbde8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x49152 and 196608x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m imgs, targets \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m      7\u001B[0m output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(imgs)\n\u001B[1;32m----> 8\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mwisdom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m (output\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32mD:\\app\\anaconda3\\envs\\lzqpytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\app\\anaconda3\\envs\\lzqpytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\app\\anaconda3\\envs\\lzqpytorch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x49152 and 196608x10)"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### linear 线性层操作\n",
    "$y = xA^T + b$，其中x为输入，A为权重，b为偏置\n",
    "对一个尺寸5*5的输入，先 reshape 成 1*25，只有一层然后与权重相乘，经过线性层，得到输出"
   ],
   "id": "6320a7622f56f42a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
